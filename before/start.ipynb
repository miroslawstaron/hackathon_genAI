{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test script for your environment\n",
    "\n",
    "This jupyter notebook helps to test the environment. It is a basic \"Hello World\" equivalent for the HuggingFace. \n",
    "\n",
    "It imports the right libraries, downloads the right model and runs a simple test.\n",
    "\n",
    "When you run this script from Visual Studio code, make sure that you use the right interpreter -- the one that is in your virtual environment. You can add it via the command palette `(Ctrl+Shift+P)` and then `Python: Select Interpreter` -- select the one that is in your virtual environment (or navigate to the right path to add it -- do NOT create a new virutal environment. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing os to ensure that we do not get too many unrelated warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
    "\n",
    "# stop warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.38.2\n"
     ]
    }
   ],
   "source": [
    "# this code checks if the transformers library is installed\n",
    "# the output should be the version of the transformers library\n",
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# the same for the torch library\n",
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# and for the tensorflow library\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller model\n",
    "\n",
    "The model below is a very small one - ca. 85 million parameters. This means that it should run on all environments. So, this model should not fail on your computer. If it does, then you need to debug your environment and you need to make it work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.36955469846725464,\n",
       "  'token': 13963,\n",
       "  'token_str': ' sliding',\n",
       "  'sequence': \"Hello I'm a sliding model.\"},\n",
       " {'score': 0.1200207844376564,\n",
       "  'token': 11430,\n",
       "  'token_str': ' working',\n",
       "  'sequence': \"Hello I'm a working model.\"},\n",
       " {'score': 0.07376236468553543,\n",
       "  'token': 1464,\n",
       "  'token_str': ' new',\n",
       "  'sequence': \"Hello I'm a new model.\"},\n",
       " {'score': 0.06340111047029495,\n",
       "  'token': 1258,\n",
       "  'token_str': ' into',\n",
       "  'sequence': \"Hello I'm a into model.\"},\n",
       " {'score': 0.03293626010417938,\n",
       "  'token': 3596,\n",
       "  'token_str': ' projective',\n",
       "  'sequence': \"Hello I'm a projective model.\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this tests a simple RoBERTa model trained by our research group on Singletons\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='mstaron/SingletonBERT')\n",
    "\n",
    "unmasker(\"Hello I'm a <mask> model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger model\n",
    "\n",
    "Below we test a bit of a larger model -- it can take a bit of time to download and run.\n",
    "\n",
    "If you do not have a GPU, you can remove the .to('cuda') line from the code below. \n",
    "\n",
    "If it does not work at all, do not worry, it may just mean that your computer is too small for the larger models. That's also fine and you can still work with the smaller models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/mnt/d/venvs/hackathon_genAI/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "# Test the function\n",
      "print(quicksort([3,6,8,10,1,2,1]))\n",
      "# Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "This algorithm works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "# tokenizer.eos_token_id is the id of <|EOT|> token\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon_genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
