{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M1oqh0F6W3ad"
   },
   "source": [
    "# Using a pre-trained model from Huggingface\n",
    "This is the model pre-trained on the Singleton examples and WolfSSL code. It is available on Huggingface.\n",
    "\n",
    "The model is a BERT model with a single layer of 768 hidden units. It was trained on the examples from the Singleton paper and the WolfSSL code. The mdoel was trained with 100 epochs and a batch size of 64, using the 2080 8GB card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the model\n",
    "\n",
    "When we use the model from Huggingface, we do not have to worry about tokenizers and other elements. We can just use the model directly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model via the huggingface library\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# load the tokenizer and the model for the pretrained SingBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the right pipeline\n",
    "\n",
    "Every model has a set of pipelines defined for it, depending on what the model can do. For example, a model that is trained for text generation will have a pipeline for text generation.\n",
    "\n",
    "The model we are using is a BERT model, which is trained for token classification. We can use the `pipeline` function to get the right pipeline for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pipeline \n",
    "from transformers import pipeline\n",
    "\n",
    "# load the pipeline for text generation\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# define the code snippet\n",
    "code_snippet = '''def HelloWorld():\n",
    "'''\n",
    "# generate the code\n",
    "generatedCode = fill_mask(code_snippet + \"return '<mask>'\")\n",
    "\n",
    "# print the suggestions for the generated code\n",
    "generatedCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's do the same for CodeParrot model\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, pipeline\n",
    "\n",
    "# load the tokenizer and the model for the pretrained SingBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained('codeparrot/codeparrot')\n",
    "\n",
    "# load the model\n",
    "model = AutoModelWithLMHead.from_pretrained(\"codeparrot/codeparrot\")\n",
    "\n",
    "# load the pipeline for text generation\n",
    "pipe = pipeline(\"text-generation\", model=\"codeparrot/codeparrot\")\n",
    "\n",
    "# define the code snippet\n",
    "code_snippet = '''def HelloWorld():\n",
    "'''\n",
    "# generate the code\n",
    "generatedCode = pipe(code_snippet)\n",
    "\n",
    "# print the suggestions for the generated code\n",
    "generatedCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the pipeline to understand code similarity\n",
    "\n",
    "We do not have to generate the code, as long as we can just generate the embeddings for the code. We can then use the embeddings to find the similarity between the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the feature extraction pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# create the pipeline, which will extract the embedding vectors\n",
    "# the models are already pre-defined, so we do not need to train anything here\n",
    "features = pipeline(\n",
    "    \"feature-extraction\",\n",
    "    model=\"microsoft/codebert-base\",\n",
    "    tokenizer=\"microsoft/codebert-base\", \n",
    "    return_tensor = False\n",
    ")\n",
    "\n",
    "# extract the features == embeddings\n",
    "lstFeatures = features('Class SingletonX1')\n",
    "\n",
    "# print the first token's embedding [CLS]\n",
    "# which is also a good approximation of the whole sentence embedding\n",
    "# the same as using np.mean(lstFeatures[0], axis=0)\n",
    "lstFeatures[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using models for code summarization\n",
    "\n",
    "This model is the CodeT5 model for summarizing source code. \n",
    "\n",
    "An example model is available here: https://huggingface.co/Salesforce/codet5-base-multi-sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667bb6679a1e4e0984161c00db43eeee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/703k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fef2cf0e8844e3b8e9441858559d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/294k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ba248018bc4ba2b28486148fd8abc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ce6e3dddb441b6a0b9460020bd1e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/12.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38264cb98c254bab879686f1589046a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb80cd5192a44a0688fcb13c51ed9344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/902 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1ea18433bc449ba2c698efd9be0095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converts a SVG string to a QImage.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base-multi-sum')\n",
    "\n",
    "text = \"\"\"def svg_to_image(string, size=None):\n",
    "if isinstance(string, unicode):\n",
    "    string = string.encode('utf-8')\n",
    "    renderer = QtSvg.QSvgRenderer(QtCore.QByteArray(string))\n",
    "if not renderer.isValid():\n",
    "    raise ValueError('Invalid SVG data.')\n",
    "if size is None:\n",
    "    size = renderer.defaultSize()\n",
    "    image = QtGui.QImage(size, QtGui.QImage.Format_ARGB32)\n",
    "    painter = QtGui.QPainter(image)\n",
    "    renderer.render(painter)\n",
    "return image\"\"\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generated_ids = model.generate(input_ids, max_length=20)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# this prints: \"Convert a SVG string to a QImage.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the pre-trained model (from our task)\n",
    "\n",
    "The model is available here: https://huggingface.co/mstaron/wolfBERTa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.20149990916252136,\n",
       "  'token': 274,\n",
       "  'token_str': ' {',\n",
       "  'sequence': 'int x = {'},\n",
       " {'score': 0.05696285516023636,\n",
       "  'token': 325,\n",
       "  'token_str': ' b',\n",
       "  'sequence': 'int x = b'},\n",
       " {'score': 0.050293806940317154,\n",
       "  'token': 265,\n",
       "  'token_str': ' 0',\n",
       "  'sequence': 'int x = 0'},\n",
       " {'score': 0.03858976066112518,\n",
       "  'token': 522,\n",
       "  'token_str': ' 2',\n",
       "  'sequence': 'int x = 2'},\n",
       " {'score': 0.030387546867132187,\n",
       "  'token': 278,\n",
       "  'token_str': ' (',\n",
       "  'sequence': 'int x = ('}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the model via the huggingface library\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# load the tokenizer and the model for the pretrained wolfBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained('mstaron/wolfBERTa')\n",
    "\n",
    "# load the model\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"mstaron/wolfBERTa\")\n",
    "\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', \n",
    "                    model=model, \n",
    "                    tokenizer=tokenizer)\n",
    "unmasker(\"int x = <mask>\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
