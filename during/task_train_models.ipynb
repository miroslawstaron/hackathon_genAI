{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M1oqh0F6W3ad"
   },
   "source": [
    "# Training an own model\n",
    "\n",
    "In this model, we can train our own model. In this example, we use an example training file: singletons_local.csv. This file contains source code of singletons that is autogenerated. The content of the singletons is not useful at all, but the structure is. \n",
    "\n",
    "After a good training, we should be able to use the fill-mask pipeline to fill the structure of the singletons. \n",
    "\n",
    "The training process is done in the following steps:\n",
    "1. Training the tokenizer\n",
    "2. Downloading a vanilla model from the Hugging Face model hub\n",
    "3. Training the model on the singletons\n",
    "4. Saving the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just checking if CUDA is available on this computer\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the standard BPE tokenizer for this workbook\n",
    "# it was described in the previous chapter of the book\n",
    "# when we discussed feature extraction\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = ['./singletons_local.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "print('Training tokenizer...')\n",
    "\n",
    "# Customize training\n",
    "# we use a large vocabulary size, but we could also do with ca. 10_000\n",
    "tokenizer.train(files=paths, \n",
    "                vocab_size=52_000, \n",
    "                min_frequency=2, \n",
    "                special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\",])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# we give this model a catchy name - wolfBERTa\n",
    "# because it is a RoBERTa model trained on the WolfSSL source code\n",
    "token_dir = './singletonBERT'\n",
    "\n",
    "if not os.path.exists(token_dir):\n",
    "  os.makedirs(token_dir)\n",
    "\n",
    "tokenizer.save_model('singletonBERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "# let's make sure that the tokenizer does not provide more tokens than we expect\n",
    "# we expect 510 tokens, because we will use the BERT model\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the RoBERTa configuration\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "# initialize the configuration\n",
    "# please note that the vocab size is the same as the one in the tokenizer. \n",
    "# if it is not, we could get exceptions that the model and the tokenizer are not compatible\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing a Model From Scratch\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# initialize the model\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but before we actually train the model\n",
    "# we need to change the tokenizer to the one that we trained\n",
    "# and to make it compatible with the tokenizer that is expected by the model\n",
    "# so we read it from the file under a different tokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# initialize the tokenizer from the file\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"./singletonBERT\", max_length=512)\n",
    "\n",
    "# please note that if we use a tokenizer that was trained before\n",
    "# the vanilla version of BPETokenizer, we will get an exception\n",
    "# that the BPE tokenizer is not collable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see if we can change this to use the Dataset library instead of the transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "new_dataset = load_dataset(\"text\", data_files='singletons_local.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's tokenize the dataset\n",
    "\n",
    "# num_proc is the argument to use all cores\n",
    "tokenized_dataset = new_dataset.map(lambda x: tokenizer(x[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training of the model requires a data collator\n",
    "# which creates a random set of tokens to mask\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we can train the model\n",
    "# by creating the trainer\n",
    "import accelerate\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./singletonBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is where we train the model\n",
    "# which corresponds to the model.fit() method in Keras\n",
    "# which we used in the previous chapters\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./singletonBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', \n",
    "                    model=model, \n",
    "                    tokenizer=tokenizer)\n",
    "\n",
    "unmasker(\"int x = <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here\n",
    "\n",
    "So, this is how you can train a model. \n",
    "\n",
    "What you can do next:\n",
    "1. Train your model on your own source code -- dump the code to a single file and train your model on it\n",
    "    a. Remember that the larger the training set, the better the model, but also the longer it takes to train it\n",
    "2. Use the model to fill the structure of your code\n",
    "    a. You can use the fill-mask pipeline to fill the structure of your code\n",
    "    b. For example, try to fill in a test code: write a structure of the test case and fill in the content of the assert() statement.\n",
    "3. Train a different model from Hugging Face\n",
    "    a. Take a look at the file `Models for Programming languages.md` and choose the right one\n",
    "    b. Remeber to choose the one that is MLM model, not XLM (cross-lingual model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
